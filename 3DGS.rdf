<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns:z="http://www.zotero.org/namespaces/export#"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:foaf="http://xmlns.com/foaf/0.1/"
 xmlns:bib="http://purl.org/net/biblio#"
 xmlns:link="http://purl.org/rss/1.0/modules/link/"
 xmlns:dcterms="http://purl.org/dc/terms/"
 xmlns:prism="http://prismstandard.org/namespaces/1.2/basic/">
    <rdf:Description rdf:about="http://arxiv.org/abs/2308.04079">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kerbl</foaf:surname>
                        <foaf:givenName>Bernhard</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kopanas</foaf:surname>
                        <foaf:givenName>Georgios</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Leimk√ºhler</foaf:surname>
                        <foaf:givenName>Thomas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Drettakis</foaf:surname>
                        <foaf:givenName>George</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_4"/>
        <link:link rdf:resource="#item_5"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Graphics</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>3D Gaussian Splatting for Real-Time Radiance Field Rendering</dc:title>
        <dcterms:abstract>Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (&gt;= 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.</dcterms:abstract>
        <dc:date>2023-08-08</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2308.04079</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:32:33</dcterms:dateSubmitted>
        <dc:description>arXiv:2308.04079</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2308.04079</dc:identifier>
        <prism:number>arXiv:2308.04079</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_4">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/4/Kerbl et al. - 2023 - 3D Gaussian Splatting for Real-Time Radiance Field Rendering.pdf"/>
        <dc:title>Preprint PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/pdf/2308.04079v1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:32:34</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_5">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/5/2308.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2308.04079</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:32:39</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2404.17774">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dai</foaf:surname>
                        <foaf:givenName>Pinxuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Jiamin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xie</foaf:surname>
                        <foaf:givenName>Wenxiang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Xinguo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Huamin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Weiwei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_8"/>
        <link:link rdf:resource="#item_9"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Graphics</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>High-quality Surface Reconstruction using Gaussian Surfels</dc:title>
        <dcterms:abstract>We propose a novel point-based representation, Gaussian surfels, to combine the advantages of the flexible optimization procedure in 3D Gaussian points and the surface alignment property of surfels. This is achieved by directly setting the z-scale of 3D Gaussian points to 0, effectively flattening the original 3D ellipsoid into a 2D ellipse. Such a design provides clear guidance to the optimizer. By treating the local z-axis as the normal direction, it greatly improves optimization stability and surface alignment. While the derivatives to the local z-axis computed from the covariance matrix are zero in this setting, we design a self-supervised normal-depth consistency loss to remedy this issue. Monocular normal priors and foreground masks are incorporated to enhance the quality of the reconstruction, mitigating issues related to highlights and background. We propose a volumetric cutting method to aggregate the information of Gaussian surfels so as to remove erroneous points in depth maps generated by alpha blending. Finally, we apply screened Poisson reconstruction method to the fused depth maps to extract the surface mesh. Experimental results show that our method demonstrates superior performance in surface reconstruction compared to state-of-the-art neural volume rendering and point-based rendering methods.</dcterms:abstract>
        <dc:date>2024-04-30</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2404.17774</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:33:18</dcterms:dateSubmitted>
        <dc:description>arXiv:2404.17774</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2404.17774</dc:identifier>
        <prism:number>arXiv:2404.17774</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_8">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/8/Dai et al. - 2024 - High-quality Surface Reconstruction using Gaussian Surfels.pdf"/>
        <dc:title>Preprint PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/pdf/2404.17774v2</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:33:31</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_9">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/9/2404.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2404.17774</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:33:36</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2311.16728">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>Huajian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Longwei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cheng</foaf:surname>
                        <foaf:givenName>Hui</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yeung</foaf:surname>
                        <foaf:givenName>Sai-Kit</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_10"/>
        <link:link rdf:resource="#item_13"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Photo-SLAM: Real-time Simultaneous Localization and Photorealistic Mapping for Monocular, Stereo, and RGB-D Cameras</dc:title>
        <dcterms:abstract>The integration of neural rendering and the SLAM system recently showed promising results in joint localization and photorealistic view reconstruction. However, existing methods, fully relying on implicit representations, are so resource-hungry that they cannot run on portable devices, which deviates from the original intention of SLAM. In this paper, we present Photo-SLAM, a novel SLAM framework with a hyper primitives map. Specifically, we simultaneously exploit explicit geometric features for localization and learn implicit photometric features to represent the texture information of the observed environment. In addition to actively densifying hyper primitives based on geometric features, we further introduce a Gaussian-Pyramid-based training method to progressively learn multi-level features, enhancing photorealistic mapping performance. The extensive experiments with monocular, stereo, and RGB-D datasets prove that our proposed system Photo-SLAM significantly outperforms current state-of-the-art SLAM systems for online photorealistic mapping, e.g., PSNR is 30% higher and rendering speed is hundreds of times faster in the Replica dataset. Moreover, the Photo-SLAM can run at real-time speed using an embedded platform such as Jetson AGX Orin, showing the potential of robotics applications.</dcterms:abstract>
        <dc:date>2024-04-08</dc:date>
        <z:shortTitle>Photo-SLAM</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2311.16728</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:33:31</dcterms:dateSubmitted>
        <dc:description>arXiv:2311.16728</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2311.16728</dc:identifier>
        <prism:number>arXiv:2311.16728</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_10">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/10/Huang et al. - 2024 - Photo-SLAM Real-time Simultaneous Localization and Photorealistic Mapping for Monocular, Stereo, an.pdf"/>
        <dc:title>Preprint PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/pdf/2311.16728v2</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:33:42</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_13">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/13/2311.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2311.16728</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:33:46</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2401.14857">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hong</foaf:surname>
                        <foaf:givenName>Sheng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>Junjie</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zheng</foaf:surname>
                        <foaf:givenName>Xinhu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zheng</foaf:surname>
                        <foaf:givenName>Chunran</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shen</foaf:surname>
                        <foaf:givenName>Shaojie</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_12"/>
        <link:link rdf:resource="#item_14"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Robotics</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>LIV-GaussMap: LiDAR-Inertial-Visual Fusion for Real-time 3D Radiance Field Map Rendering</dc:title>
        <dcterms:abstract>We introduce an integrated precise LiDAR, Inertial, and Visual (LIV) multimodal sensor fused mapping system that builds on the differentiable \pre{surface splatting }\now{Gaussians} to improve the mapping fidelity, quality, and structural accuracy. Notably, this is also a novel form of tightly coupled map for LiDAR-visual-inertial sensor fusion. This system leverages the complementary characteristics of LiDAR and visual data to capture the geometric structures of large-scale 3D scenes and restore their visual surface information with high fidelity. The initialization for the scene's surface Gaussians and the sensor's poses of each frame are obtained using a LiDAR-inertial system with the feature of size-adaptive voxels. Then, we optimized and refined the Gaussians using visual-derived photometric gradients to optimize their quality and density. Our method is compatible with various types of LiDAR, including solid-state and mechanical LiDAR, supporting both repetitive and non-repetitive scanning modes. Bolstering structure construction through LiDAR and facilitating real-time generation of photorealistic renderings across diverse LIV datasets. It showcases notable resilience and versatility in generating real-time photorealistic scenes potentially for digital twins and virtual reality, while also holding potential applicability in real-time SLAM and robotics domains. We release our software and hardware and self-collected datasets to benefit the community.</dcterms:abstract>
        <dc:date>2024-05-17</dc:date>
        <z:shortTitle>LIV-GaussMap</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2401.14857</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:33:42</dcterms:dateSubmitted>
        <dc:description>arXiv:2401.14857</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2401.14857</dc:identifier>
        <prism:number>arXiv:2401.14857</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_12">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/12/Hong et al. - 2024 - LIV-GaussMap LiDAR-Inertial-Visual Fusion for Real-time 3D Radiance Field Map Rendering.pdf"/>
        <dc:title>Preprint PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/pdf/2401.14857v2</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:33:45</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_14">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/14/2401.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2401.14857</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:33:50</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2203.00893">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zheng</foaf:surname>
                        <foaf:givenName>Chunran</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhu</foaf:surname>
                        <foaf:givenName>Qingyan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Wei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Xiyuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Guo</foaf:surname>
                        <foaf:givenName>Qizhi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Fu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_16"/>
        <link:link rdf:resource="#item_17"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Robotics</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>FAST-LIVO: Fast and Tightly-coupled Sparse-Direct LiDAR-Inertial-Visual Odometry</dc:title>
        <dcterms:abstract>To achieve accurate and robust pose estimation in Simultaneous Localization and Mapping (SLAM) task, multi-sensor fusion is proven to be an effective solution and thus provides great potential in robotic applications. This paper proposes FAST-LIVO, a fast LiDAR-Inertial-Visual Odometry system, which builds on two tightly-coupled and direct odometry subsystems: a VIO subsystem and a LIO subsystem. The LIO subsystem registers raw points (instead of feature points on e.g., edges or planes) of a new scan to an incrementally-built point cloud map. The map points are additionally attached with image patches, which are then used in the VIO subsystem to align a new image by minimizing the direct photometric errors without extracting any visual features (e.g., ORB or FAST corner features). To further improve the VIO robustness and accuracy, a novel outlier rejection method is proposed to reject unstable map points that lie on edges or are occluded in the image view. Experiments on both open data sequences and our customized device data are conducted. The results show our proposed system outperforms other counterparts and can handle challenging environments at reduced computation cost. The system supports both multi-line spinning LiDARs and emerging solid-state LiDARs with completely different scanning patterns, and can run in real-time on both Intel and ARM processors. We open source our code and dataset of this work on Github to benefit the robotics community.</dcterms:abstract>
        <dc:date>2022-03-02</dc:date>
        <z:shortTitle>FAST-LIVO</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2203.00893</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:33:54</dcterms:dateSubmitted>
        <dc:description>arXiv:2203.00893</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2203.00893</dc:identifier>
        <prism:number>arXiv:2203.00893</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_16">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/16/Zheng et al. - 2022 - FAST-LIVO Fast and Tightly-coupled Sparse-Direct LiDAR-Inertial-Visual Odometry.pdf"/>
        <dc:title>Preprint PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/pdf/2203.00893v1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:33:55</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_17">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/17/2203.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2203.00893</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:34:00</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2107.06829">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Wei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cai</foaf:surname>
                        <foaf:givenName>Yixi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>Dongjiao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lin</foaf:surname>
                        <foaf:givenName>Jiarong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Fu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_19"/>
        <link:link rdf:resource="#item_20"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Robotics</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>FAST-LIO2: Fast Direct LiDAR-inertial Odometry</dc:title>
        <dcterms:abstract>This paper presents FAST-LIO2: a fast, robust, and versatile LiDAR-inertial odometry framework. Building on a highly efficient tightly-coupled iterated Kalman filter, FAST-LIO2 has two key novelties that allow fast, robust, and accurate LiDAR navigation (and mapping). The first one is directly registering raw points to the map (and subsequently update the map, i.e., mapping) without extracting features. This enables the exploitation of subtle features in the environment and hence increases the accuracy. The elimination of a hand-engineered feature extraction module also makes it naturally adaptable to emerging LiDARs of different scanning patterns; The second main novelty is maintaining a map by an incremental k-d tree data structure, ikd-Tree, that enables incremental updates (i.e., point insertion, delete) and dynamic re-balancing. Compared with existing dynamic data structures (octree, R*-tree, nanoflann k-d tree), ikd-Tree achieves superior overall performance while naturally supports downsampling on the tree. We conduct an exhaustive benchmark comparison in 19 sequences from a variety of open LiDAR datasets. FAST-LIO2 achieves consistently higher accuracy at a much lower computation load than other state-of-the-art LiDAR-inertial navigation systems. Various real-world experiments on solid-state LiDARs with small FoV are also conducted. Overall, FAST-LIO2 is computationally-efficient (e.g., up to 100 Hz odometry and mapping in large outdoor environments), robust (e.g., reliable pose estimation in cluttered indoor environments with rotation up to 1000 deg/s), versatile (i.e., applicable to both multi-line spinning and solid-state LiDARs, UAV and handheld platforms, and Intel and ARM-based processors), while still achieving higher accuracy than existing methods. Our implementation of the system FAST-LIO2, and the data structure ikd-Tree are both open-sourced on Github.</dcterms:abstract>
        <dc:date>2021-07-14</dc:date>
        <z:shortTitle>FAST-LIO2</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2107.06829</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:34:02</dcterms:dateSubmitted>
        <dc:description>arXiv:2107.06829</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2107.06829</dc:identifier>
        <prism:number>arXiv:2107.06829</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_19">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/19/Xu et al. - 2021 - FAST-LIO2 Fast Direct LiDAR-inertial Odometry.pdf"/>
        <dc:title>Preprint PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/pdf/2107.06829v1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:34:09</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_20">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/20/2107.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2107.06829</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:34:13</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2408.14035">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zheng</foaf:surname>
                        <foaf:givenName>Chunran</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Wei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zou</foaf:surname>
                        <foaf:givenName>Zuhao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hua</foaf:surname>
                        <foaf:givenName>Tong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yuan</foaf:surname>
                        <foaf:givenName>Chongjian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>Dongjiao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>Bingyang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Zheng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lin</foaf:surname>
                        <foaf:givenName>Jiarong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhu</foaf:surname>
                        <foaf:givenName>Fangcheng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ren</foaf:surname>
                        <foaf:givenName>Yunfan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Rong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Meng</foaf:surname>
                        <foaf:givenName>Fanle</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Fu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_22"/>
        <link:link rdf:resource="#item_24"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Robotics</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>FAST-LIVO2: Fast, Direct LiDAR-Inertial-Visual Odometry</dc:title>
        <dcterms:abstract>This paper proposes FAST-LIVO2: a fast, direct LiDAR-inertial-visual odometry framework to achieve accurate and robust state estimation in SLAM tasks and provide great potential in real-time, onboard robotic applications. FAST-LIVO2 fuses the IMU, LiDAR and image measurements efficiently through an ESIKF. To address the dimension mismatch between the heterogeneous LiDAR and image measurements, we use a sequential update strategy in the Kalman filter. To enhance the efficiency, we use direct methods for both the visual and LiDAR fusion, where the LiDAR module registers raw points without extracting edge or plane features and the visual module minimizes direct photometric errors without extracting ORB or FAST corner features. The fusion of both visual and LiDAR measurements is based on a single unified voxel map where the LiDAR module constructs the geometric structure for registering new LiDAR scans and the visual module attaches image patches to the LiDAR points. To enhance the accuracy of image alignment, we use plane priors from the LiDAR points in the voxel map (and even refine the plane prior) and update the reference patch dynamically after new images are aligned. Furthermore, to enhance the robustness of image alignment, FAST-LIVO2 employs an on-demanding raycast operation and estimates the image exposure time in real time. Lastly, we detail three applications of FAST-LIVO2: UAV onboard navigation demonstrating the system's computation efficiency for real-time onboard navigation, airborne mapping showcasing the system's mapping accuracy, and 3D model rendering (mesh-based and NeRF-based) underscoring the suitability of our reconstructed dense map for subsequent rendering tasks. We open source our code, dataset and application on GitHub to benefit the robotics community.</dcterms:abstract>
        <dc:date>2024-08-28</dc:date>
        <z:shortTitle>FAST-LIVO2</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2408.14035</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:34:19</dcterms:dateSubmitted>
        <dc:description>arXiv:2408.14035</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2408.14035</dc:identifier>
        <prism:number>arXiv:2408.14035</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_22">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/22/Zheng et al. - 2024 - FAST-LIVO2 Fast, Direct LiDAR-Inertial-Visual Odometry.pdf"/>
        <dc:title>Preprint PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/pdf/2408.14035v2</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:34:25</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_24">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/24/2408.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2408.14035</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:34:30</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1711.01691">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Park</foaf:surname>
                        <foaf:givenName>Chanoh</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Moghadam</foaf:surname>
                        <foaf:givenName>Peyman</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kim</foaf:surname>
                        <foaf:givenName>Soohwan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Elfes</foaf:surname>
                        <foaf:givenName>Alberto</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fookes</foaf:surname>
                        <foaf:givenName>Clinton</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sridharan</foaf:surname>
                        <foaf:givenName>Sridha</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_25"/>
        <link:link rdf:resource="#item_26"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Robotics</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Elastic LiDAR Fusion: Dense Map-Centric Continuous-Time SLAM</dc:title>
        <dcterms:abstract>The concept of continuous-time trajectory representation has brought increased accuracy and efficiency to multi-modal sensor fusion in modern SLAM. However, regardless of these advantages, its offline property caused by the requirement of global batch optimization is critically hindering its relevance for real-time and life-long applications. In this paper, we present a dense map-centric SLAM method based on a continuous-time trajectory to cope with this problem. The proposed system locally functions in a similar fashion to conventional Continuous-Time SLAM (CT-SLAM). However, it removes the need for global trajectory optimization by introducing map deformation. The computational complexity of the proposed approach for loop closure does not depend on the operation time, but only on the size of the space it explored before the loop closure. It is therefore more suitable for long term operation compared to the conventional CT-SLAM. Furthermore, the proposed method reduces uncertainty in the reconstructed dense map by using probabilistic surface element (surfel) fusion. We demonstrate that the proposed method produces globally consistent maps without global batch trajectory optimization, and effectively reduces LiDAR noise by surfel fusion.</dcterms:abstract>
        <dc:date>2018-03-05</dc:date>
        <z:shortTitle>Elastic LiDAR Fusion</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1711.01691</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:34:29</dcterms:dateSubmitted>
        <dc:description>arXiv:1711.01691</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.1711.01691</dc:identifier>
        <prism:number>arXiv:1711.01691</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_25">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/25/Park et al. - 2018 - Elastic LiDAR Fusion Dense Map-Centric Continuous-Time SLAM.pdf"/>
        <dc:title>Preprint PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/pdf/1711.01691v3</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:34:32</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_26">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/26/1711.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/1711.01691</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:34:37</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2008.02274">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Park</foaf:surname>
                        <foaf:givenName>Chanoh</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Moghadam</foaf:surname>
                        <foaf:givenName>Peyman</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Williams</foaf:surname>
                        <foaf:givenName>Jason</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kim</foaf:surname>
                        <foaf:givenName>Soohwan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sridharan</foaf:surname>
                        <foaf:givenName>Sridha</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fookes</foaf:surname>
                        <foaf:givenName>Clinton</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_28"/>
        <link:link rdf:resource="#item_29"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Robotics</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Elasticity Meets Continuous-Time: Map-Centric Dense 3D LiDAR SLAM</dc:title>
        <dcterms:abstract>Map-centric SLAM utilizes elasticity as a means of loop closure. This approach reduces the cost of loop closure while still provides large-scale fusion-based dense maps, when compared to the trajectory-centric SLAM approaches. In this paper, we present a novel framework for 3D LiDAR-based map-centric SLAM. Having the advantages of a map-centric approach, our method exhibits new features to overcome the shortcomings of existing systems, associated with multi-modal sensor fusion and LiDAR motion distortion. This is accomplished through the use of a local Continuous-Time (CT) trajectory representation. Also, our surface resolution preservative matching algorithm and Wishart-based surfel fusion model enables non-redundant yet dense mapping. Furthermore, we present a robust metric loop closure model to make the approach stable regardless of where the loop closure occurs. Finally, we demonstrate our approach through both simulation and real data experiments using multiple sensor payload configurations and environments to illustrate its utility and robustness.</dcterms:abstract>
        <dc:date>2020-08-05</dc:date>
        <z:shortTitle>Elasticity Meets Continuous-Time</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2008.02274</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:34:41</dcterms:dateSubmitted>
        <dc:description>arXiv:2008.02274</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2008.02274</dc:identifier>
        <prism:number>arXiv:2008.02274</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_28">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/28/Park et al. - 2020 - Elasticity Meets Continuous-Time Map-Centric Dense 3D LiDAR SLAM.pdf"/>
        <dc:title>Preprint PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/pdf/2008.02274v1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:34:48</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_29">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/29/2008.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2008.02274</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:34:53</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1709.01265">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Park</foaf:surname>
                        <foaf:givenName>Chanoh</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kim</foaf:surname>
                        <foaf:givenName>Soohwan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Moghadam</foaf:surname>
                        <foaf:givenName>Peyman</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fookes</foaf:surname>
                        <foaf:givenName>Clinton</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sridharan</foaf:surname>
                        <foaf:givenName>Sridha</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_31"/>
        <link:link rdf:resource="#item_32"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Robotics</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Probabilistic Surfel Fusion for Dense LiDAR Mapping</dc:title>
        <dcterms:abstract>With the recent development of high-end LiDARs, more and more systems are able to continuously map the environment while moving and producing spatially redundant information. However, none of the previous approaches were able to effectively exploit this redundancy in a dense LiDAR mapping problem. In this paper, we present a new approach for dense LiDAR mapping using probabilistic surfel fusion. The proposed system is capable of reconstructing a high-quality dense surface element (surfel) map from spatially redundant multiple views. This is achieved by a proposed probabilistic surfel fusion along with a geometry considered data association. The proposed surfel data association method considers surface resolution as well as high measurement uncertainty along its beam direction which enables the mapping system to be able to control surface resolution without introducing spatial digitization. The proposed fusion method successfully suppresses the map noise level by considering measurement noise caused by laser beam incident angle and depth distance in a Bayesian filtering framework. Experimental results with simulated and real data for the dense surfel mapping prove the ability of the proposed method to accurately find the canonical form of the environment without further post-processing.</dcterms:abstract>
        <dc:date>2017-09-17</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1709.01265</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:35:00</dcterms:dateSubmitted>
        <dc:description>arXiv:1709.01265</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.1709.01265</dc:identifier>
        <prism:number>arXiv:1709.01265</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_31">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/31/Park et al. - 2017 - Probabilistic Surfel Fusion for Dense LiDAR Mapping.pdf"/>
        <dc:title>Preprint PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/pdf/1709.01265v2</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:35:06</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_32">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/32/1709.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/1709.01265</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:35:10</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2403.17888">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>Binbin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yu</foaf:surname>
                        <foaf:givenName>Zehao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Anpei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Geiger</foaf:surname>
                        <foaf:givenName>Andreas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gao</foaf:surname>
                        <foaf:givenName>Shenghua</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_34"/>
        <link:link rdf:resource="#item_35"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Graphics</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>2D Gaussian Splatting for Geometrically Accurate Radiance Fields</dc:title>
        <dcterms:abstract>3D Gaussian Splatting (3DGS) has recently revolutionized radiance field reconstruction, achieving high quality novel view synthesis and fast rendering speed without baking. However, 3DGS fails to accurately represent surfaces due to the multi-view inconsistent nature of 3D Gaussians. We present 2D Gaussian Splatting (2DGS), a novel approach to model and reconstruct geometrically accurate radiance fields from multi-view images. Our key idea is to collapse the 3D volume into a set of 2D oriented planar Gaussian disks. Unlike 3D Gaussians, 2D Gaussians provide view-consistent geometry while modeling surfaces intrinsically. To accurately recover thin surfaces and achieve stable optimization, we introduce a perspective-correct 2D splatting process utilizing ray-splat intersection and rasterization. Additionally, we incorporate depth distortion and normal consistency terms to further enhance the quality of the reconstructions. We demonstrate that our differentiable renderer allows for noise-free and detailed geometry reconstruction while maintaining competitive appearance quality, fast training speed, and real-time rendering.</dcterms:abstract>
        <dc:date>2024-06-09</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2403.17888</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:35:18</dcterms:dateSubmitted>
        <dc:description>arXiv:2403.17888</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2403.17888</dc:identifier>
        <prism:number>arXiv:2403.17888</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_34">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/34/Huang et al. - 2024 - 2D Gaussian Splatting for Geometrically Accurate Radiance Fields.pdf"/>
        <dc:title>Preprint PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/pdf/2403.17888v2</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:35:20</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_35">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/35/2403.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2403.17888</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:35:25</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2403.12550">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ha</foaf:surname>
                        <foaf:givenName>Seongbo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yeon</foaf:surname>
                        <foaf:givenName>Jiung</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yu</foaf:surname>
                        <foaf:givenName>Hyeonwoo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_37"/>
        <link:link rdf:resource="#item_38"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>RGBD GS-ICP SLAM</dc:title>
        <dcterms:abstract>Simultaneous Localization and Mapping (SLAM) with dense representation plays a key role in robotics, Virtual Reality (VR), and Augmented Reality (AR) applications. Recent advancements in dense representation SLAM have highlighted the potential of leveraging neural scene representation and 3D Gaussian representation for high-fidelity spatial representation. In this paper, we propose a novel dense representation SLAM approach with a fusion of Generalized Iterative Closest Point (G-ICP) and 3D Gaussian Splatting (3DGS). In contrast to existing methods, we utilize a single Gaussian map for both tracking and mapping, resulting in mutual benefits. Through the exchange of covariances between tracking and mapping processes with scale alignment techniques, we minimize redundant computations and achieve an efficient system. Additionally, we enhance tracking accuracy and mapping quality through our keyframe selection methods. Experimental results demonstrate the effectiveness of our approach, showing an incredibly fast speed up to 107 FPS (for the entire system) and superior quality of the reconstructed map.</dcterms:abstract>
        <dc:date>2024-03-22</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2403.12550</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:35:36</dcterms:dateSubmitted>
        <dc:description>arXiv:2403.12550</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2403.12550</dc:identifier>
        <prism:number>arXiv:2403.12550</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_37">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/37/Ha et al. - 2024 - RGBD GS-ICP SLAM.pdf"/>
        <dc:title>Preprint PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/pdf/2403.12550v2</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:35:46</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_38">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/38/2403.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2403.12550</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:35:51</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Document rdf:about="https://arxiv.org/html/2411.12185v1">
        <z:itemType>webpage</z:itemType>
        <dcterms:isPartOf>
           <z:Website></z:Website>
        </dcterms:isPartOf>
        <link:link rdf:resource="#item_40"/>
        <dc:title>LiV-GS: LiDAR-Vision Integration for 3D Gaussian Splatting SLAM in Outdoor Environments</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/html/2411.12185v1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:47:42</dcterms:dateSubmitted>
    </bib:Document>
    <z:Attachment rdf:about="#item_40">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/40/2411.html"/>
        <dc:title>LiV-GS: LiDAR-Vision Integration for 3D Gaussian Splatting SLAM in Outdoor Environments</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/html/2411.12185v1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-21 13:47:51</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://ieeexplore.ieee.org/document/6906584">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2014 IEEE International Conference on Robotics and Automation (ICRA)</dc:title>
                <dc:identifier>DOI 10.1109/ICRA.2014.6906584</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Forster</foaf:surname>
                        <foaf:givenName>Christian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pizzoli</foaf:surname>
                        <foaf:givenName>Matia</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Scaramuzza</foaf:surname>
                        <foaf:givenName>Davide</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_42"/>
        <link:link rdf:resource="#item_43"/>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Cameras</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Feature extraction</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Motion estimation</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Optimization</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Robustness</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Three-dimensional displays</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Tracking</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:title>SVO: Fast semi-direct monocular visual odometry</dc:title>
        <dcterms:abstract>We propose a semi-direct monocular visual odometry algorithm that is precise, robust, and faster than current state-of-the-art methods. The semi-direct approach eliminates the need of costly feature extraction and robust matching techniques for motion estimation. Our algorithm operates directly on pixel intensities, which results in subpixel precision at high frame-rates. A probabilistic mapping method that explicitly models outlier measurements is used to estimate 3D points, which results in fewer outliers and more reliable points. Precise and high frame-rate motion estimation brings increased robustness in scenes of little, repetitive, and high-frequency texture. The algorithm is applied to micro-aerial-vehicle state-estimation in GPS-denied environments and runs at 55 frames per second on the onboard embedded computer and at more than 300 frames per second on a consumer laptop. We call our approach SVO (Semi-direct Visual Odometry) and release our implementation as open-source software.</dcterms:abstract>
        <dc:date>2014-05</dc:date>
        <z:shortTitle>SVO</z:shortTitle>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://ieeexplore.ieee.org/document/6906584</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-22 06:37:35</dcterms:dateSubmitted>
        <dc:description>ISSN: 1050-4729</dc:description>
        <bib:pages>15-22</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2014 IEEE International Conference on Robotics and Automation (ICRA)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_42">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/42/Forster et al. - 2014 - SVO Fast semi-direct monocular visual odometry.pdf"/>
        <dc:title>Accepted Version</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.zora.uzh.ch/id/eprint/125453/1/ICRA14_Forster.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-22 06:37:40</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_43">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/43/6906584.html"/>
        <dc:title>IEEE Xplore Abstract Record</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://ieeexplore.ieee.org/document/6906584</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-22 06:37:44</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2411.15476">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wen</foaf:surname>
                        <foaf:givenName>Long</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Shixin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Yu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>Yuhong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lin</foaf:surname>
                        <foaf:givenName>Jianjie</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pan</foaf:surname>
                        <foaf:givenName>Fengjunjie</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bing</foaf:surname>
                        <foaf:givenName>Zhenshan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Knoll</foaf:surname>
                        <foaf:givenName>Alois</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_45"/>
        <link:link rdf:resource="#item_46"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Robotics</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Gassidy: Gaussian Splatting SLAM in Dynamic Environments</dc:title>
        <dcterms:abstract>3D Gaussian Splatting (3DGS) allows flexible adjustments to scene representation, enabling continuous optimization of scene quality during dense visual simultaneous localization and mapping (SLAM) in static environments. However, 3DGS faces challenges in handling environmental disturbances from dynamic objects with irregular movement, leading to degradation in both camera tracking accuracy and map reconstruction quality. To address this challenge, we develop an RGB-D dense SLAM which is called Gaussian Splatting SLAM in Dynamic Environments (Gassidy). This approach calculates Gaussians to generate rendering loss flows for each environmental component based on a designed photometric-geometric loss function. To distinguish and filter environmental disturbances, we iteratively analyze rendering loss flows to detect features characterized by changes in loss values between dynamic objects and static components. This process ensures a clean environment for accurate scene reconstruction. Compared to state-of-the-art SLAM methods, experimental results on open datasets show that Gassidy improves camera tracking precision by up to 97.9% and enhances map quality by up to 6%.</dcterms:abstract>
        <dc:date>2024-11-23</dc:date>
        <z:shortTitle>Gassidy</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2411.15476</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-27 02:50:57</dcterms:dateSubmitted>
        <dc:description>arXiv:2411.15476</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2411.15476</dc:identifier>
        <prism:number>arXiv:2411.15476</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_45">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/45/Wen et al. - 2024 - Gassidy Gaussian Splatting SLAM in Dynamic Environments.pdf"/>
        <dc:title>Preprint PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/pdf/2411.15476v1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-27 02:51:09</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_46">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/46/2411.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2411.15476</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-27 02:51:14</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2411.15800">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Haoang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Meng</foaf:surname>
                        <foaf:givenName>Xiangqi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zuo</foaf:surname>
                        <foaf:givenName>Xingxing</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Zhe</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Hesheng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cremers</foaf:surname>
                        <foaf:givenName>Daniel</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_51"/>
        <link:link rdf:resource="#item_52"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Robotics</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>PG-SLAM: Photo-realistic and Geometry-aware RGB-D SLAM in Dynamic Environments</dc:title>
        <dcterms:abstract>Simultaneous localization and mapping (SLAM) has achieved impressive performance in static environments. However, SLAM in dynamic environments remains an open question. Many methods directly filter out dynamic objects, resulting in incomplete scene reconstruction and limited accuracy of camera localization. The other works express dynamic objects by point clouds, sparse joints, or coarse meshes, which fails to provide a photo-realistic representation. To overcome the above limitations, we propose a photo-realistic and geometry-aware RGB-D SLAM method by extending Gaussian splatting. Our method is composed of three main modules to 1) map the dynamic foreground including non-rigid humans and rigid items, 2) reconstruct the static background, and 3) localize the camera. To map the foreground, we focus on modeling the deformations and/or motions. We consider the shape priors of humans and exploit geometric and appearance constraints of humans and items. For background mapping, we design an optimization strategy between neighboring local maps by integrating appearance constraint into geometric alignment. As to camera localization, we leverage both static background and dynamic foreground to increase the observations for noise compensation. We explore the geometric and appearance constraints by associating 3D Gaussians with 2D optical flows and pixel patches. Experiments on various real-world datasets demonstrate that our method outperforms state-of-the-art approaches in terms of camera localization and scene representation. Source codes will be publicly available upon paper acceptance.</dcterms:abstract>
        <dc:date>2024-11-24</dc:date>
        <z:shortTitle>PG-SLAM</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2411.15800</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-27 02:53:11</dcterms:dateSubmitted>
        <dc:description>arXiv:2411.15800</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2411.15800</dc:identifier>
        <prism:number>arXiv:2411.15800</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_51">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/51/Li et al. - 2024 - PG-SLAM Photo-realistic and Geometry-aware RGB-D SLAM in Dynamic Environments.pdf"/>
        <dc:title>Preprint PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/pdf/2411.15800v1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-27 02:53:12</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_52">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/52/2411.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2411.15800</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-27 02:53:17</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2411.16816">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hess</foaf:surname>
                        <foaf:givenName>Georg</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lindstr√∂m</foaf:surname>
                        <foaf:givenName>Carl</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fatemi</foaf:surname>
                        <foaf:givenName>Maryam</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Petersson</foaf:surname>
                        <foaf:givenName>Christoffer</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Svensson</foaf:surname>
                        <foaf:givenName>Lennart</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_57"/>
        <link:link rdf:resource="#item_58"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Graphics</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>SplatAD: Real-Time Lidar and Camera Rendering with 3D Gaussian Splatting for Autonomous Driving</dc:title>
        <dcterms:abstract>Ensuring the safety of autonomous robots, such as self-driving vehicles, requires extensive testing across diverse driving scenarios. Simulation is a key ingredient for conducting such testing in a cost-effective and scalable way. Neural rendering methods have gained popularity, as they can build simulation environments from collected logs in a data-driven manner. However, existing neural radiance field (NeRF) methods for sensor-realistic rendering of camera and lidar data suffer from low rendering speeds, limiting their applicability for large-scale testing. While 3D Gaussian Splatting (3DGS) enables real-time rendering, current methods are limited to camera data and are unable to render lidar data essential for autonomous driving. To address these limitations, we propose SplatAD, the first 3DGS-based method for realistic, real-time rendering of dynamic scenes for both camera and lidar data. SplatAD accurately models key sensor-specific phenomena such as rolling shutter effects, lidar intensity, and lidar ray dropouts, using purpose-built algorithms to optimize rendering efficiency. Evaluation across three autonomous driving datasets demonstrates that SplatAD achieves state-of-the-art rendering quality with up to +2 PSNR for NVS and +3 PSNR for reconstruction while increasing rendering speed over NeRF-based methods by an order of magnitude. See https://research.zenseact.com/publications/splatad/ for our project page.</dcterms:abstract>
        <dc:date>2024-11-25</dc:date>
        <z:shortTitle>SplatAD</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2411.16816</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-28 02:32:51</dcterms:dateSubmitted>
        <dc:description>arXiv:2411.16816</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2411.16816</dc:identifier>
        <prism:number>arXiv:2411.16816</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_57">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/57/Hess et al. - 2024 - SplatAD Real-Time Lidar and Camera Rendering with 3D Gaussian Splatting for Autonomous Driving.pdf"/>
        <dc:title>Preprint PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/pdf/2411.16816v1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-28 02:32:53</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_58">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/58/2411.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2411.16816</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-28 02:32:58</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2411.16785">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yugay</foaf:surname>
                        <foaf:givenName>Vladimir</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gevers</foaf:surname>
                        <foaf:givenName>Theo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Oswald</foaf:surname>
                        <foaf:givenName>Martin R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_60"/>
        <link:link rdf:resource="#item_61"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Robotics</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Artificial Intelligence</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>MAGiC-SLAM: Multi-Agent Gaussian Globally Consistent SLAM</dc:title>
        <dcterms:abstract>Simultaneous localization and mapping (SLAM) systems with novel view synthesis capabilities are widely used in computer vision, with applications in augmented reality, robotics, and autonomous driving. However, existing approaches are limited to single-agent operation. Recent work has addressed this problem using a distributed neural scene representation. Unfortunately, existing methods are slow, cannot accurately render real-world data, are restricted to two agents, and have limited tracking accuracy. In contrast, we propose a rigidly deformable 3D Gaussian-based scene representation that dramatically speeds up the system. However, improving tracking accuracy and reconstructing a globally consistent map from multiple agents remains challenging due to trajectory drift and discrepancies across agents' observations. Therefore, we propose new tracking and map-merging mechanisms and integrate loop closure in the Gaussian-based SLAM pipeline. We evaluate MAGiC-SLAM on synthetic and real-world datasets and find it more accurate and faster than the state of the art.</dcterms:abstract>
        <dc:date>2024-11-25</dc:date>
        <z:shortTitle>MAGiC-SLAM</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2411.16785</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-28 02:48:19</dcterms:dateSubmitted>
        <dc:description>arXiv:2411.16785</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2411.16785</dc:identifier>
        <prism:number>arXiv:2411.16785</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_60">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/60/Yugay et al. - 2024 - MAGiC-SLAM Multi-Agent Gaussian Globally Consistent SLAM.pdf"/>
        <dc:title>Preprint PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/pdf/2411.16785v1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-28 02:48:20</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_61">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/61/2411.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2411.16785</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-11-28 02:48:24</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2312.02126">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Keetha</foaf:surname>
                        <foaf:givenName>Nikhil</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Karhade</foaf:surname>
                        <foaf:givenName>Jay</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jatavallabhula</foaf:surname>
                        <foaf:givenName>Krishna Murthy</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Gengshan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Scherer</foaf:surname>
                        <foaf:givenName>Sebastian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ramanan</foaf:surname>
                        <foaf:givenName>Deva</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Luiten</foaf:surname>
                        <foaf:givenName>Jonathon</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_63"/>
        <link:link rdf:resource="#item_64"/>
        <link:link rdf:resource="#item_65"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Artificial Intelligence</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Robotics</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>SplaTAM: Splat, Track &amp; Map 3D Gaussians for Dense RGB-D SLAM</dc:title>
        <dcterms:abstract>Dense simultaneous localization and mapping (SLAM) is crucial for robotics and augmented reality applications. However, current methods are often hampered by the non-volumetric or implicit way they represent a scene. This work introduces SplaTAM, an approach that, for the first time, leverages explicit volumetric representations, i.e., 3D Gaussians, to enable high-fidelity reconstruction from a single unposed RGB-D camera, surpassing the capabilities of existing methods. SplaTAM employs a simple online tracking and mapping system tailored to the underlying Gaussian representation. It utilizes a silhouette mask to elegantly capture the presence of scene density. This combination enables several benefits over prior representations, including fast rendering and dense optimization, quickly determining if areas have been previously mapped, and structured map expansion by adding more Gaussians. Extensive experiments show that SplaTAM achieves up to 2x superior performance in camera pose estimation, map construction, and novel-view synthesis over existing methods, paving the way for more immersive high-fidelity SLAM applications.</dcterms:abstract>
        <dc:date>2024-04-16</dc:date>
        <z:shortTitle>SplaTAM</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2312.02126</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-15 13:32:00</dcterms:dateSubmitted>
        <dc:description>arXiv:2312.02126 [cs]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2312.02126</dc:identifier>
        <prism:number>arXiv:2312.02126</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_63">
        <rdf:value>Comment: CVPR 2024. Website: https://spla-tam.github.io/</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_64">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/64/Keetha et al. - 2024 - SplaTAM Splat, Track &amp; Map 3D Gaussians for Dense RGB-D SLAM.pdf"/>
        <dc:title>Preprint PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/pdf/2312.02126v3</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-15 13:32:13</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_65">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/65/2312.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2312.02126</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-15 13:32:17</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2312.06741">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Matsuki</foaf:surname>
                        <foaf:givenName>Hidenobu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Murai</foaf:surname>
                        <foaf:givenName>Riku</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kelly</foaf:surname>
                        <foaf:givenName>Paul H. J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Davison</foaf:surname>
                        <foaf:givenName>Andrew J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_67"/>
        <link:link rdf:resource="#item_68"/>
        <link:link rdf:resource="#item_69"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Robotics</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Gaussian Splatting SLAM</dc:title>
        <dcterms:abstract>We present the first application of 3D Gaussian Splatting in monocular SLAM, the most fundamental but the hardest setup for Visual SLAM. Our method, which runs live at 3fps, utilises Gaussians as the only 3D representation, unifying the required representation for accurate, efficient tracking, mapping, and high-quality rendering. Designed for challenging monocular settings, our approach is seamlessly extendable to RGB-D SLAM when an external depth sensor is available. Several innovations are required to continuously reconstruct 3D scenes with high fidelity from a live camera. First, to move beyond the original 3DGS algorithm, which requires accurate poses from an offline Structure from Motion (SfM) system, we formulate camera tracking for 3DGS using direct optimisation against the 3D Gaussians, and show that this enables fast and robust tracking with a wide basin of convergence. Second, by utilising the explicit nature of the Gaussians, we introduce geometric verification and regularisation to handle the ambiguities occurring in incremental 3D dense reconstruction. Finally, we introduce a full SLAM system which not only achieves state-of-the-art results in novel view synthesis and trajectory estimation but also reconstruction of tiny and even transparent objects.</dcterms:abstract>
        <dc:date>2024-04-14</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2312.06741</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-15 13:32:44</dcterms:dateSubmitted>
        <dc:description>arXiv:2312.06741 [cs]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2312.06741</dc:identifier>
        <prism:number>arXiv:2312.06741</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_67">
        <rdf:value>Comment: CVPR2024 Highlight. First two authors contributed equally to this work. Project Page: https://rmurai.co.uk/projects/GaussianSplattingSLAM/</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_68">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/68/Matsuki et al. - 2024 - Gaussian Splatting SLAM.pdf"/>
        <dc:title>Preprint PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/pdf/2312.06741v2</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-15 13:33:00</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_69">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/69/2312.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2312.06741</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-15 13:33:04</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
</rdf:RDF>
